<!DOCTYPE html>
<!-- saved from url=(0033)https://QicongXie.github.io/end2endvc/ -->
<html lang="en-US">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">


  <!-- Begin Jekyll SEO tag v2.7.1 -->
  <title>DualVC 3: Leveraging Language Model Generated Pseudo Context for End-to-end Low Latency Streaming Voice Conversion</title>
  <meta name="generator" content="Jekyll v3.9.0">
  <meta property="og:title" content="TODO: title">
  <meta property="og:locale" content="en_US">
  <meta name="twitter:card" content="summary">
  <!-- End Jekyll SEO tag -->

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="theme-color" content="#157878">
  <link rel="stylesheet" href="style.css">
</head>

<body data-new-gr-c-s-check-loaded="14.1001.0" data-gr-ext-installed="">
  <section class="page-header">
    <!-- <h1 class="project-name">Demo PAGE</h1> -->
    <!-- <h2 class="project-tagline"></h2> -->


  </section>

  <section class="main-content">
    <h1 id="">
      <center>DualVC 3: Leveraging Language Model Generated Pseudo Context for End-to-end Low Latency Streaming Voice Conversion</center>
    </h1>

    <center>Ziqian Ning<sup>1, 2</sup>, Shuai Wang<sup>3</sup>, Pengcheng Zhu<sup>2</sup>, Zhichao Wang<sup>1</sup>, Jixun Yao<sup>1</sup>, Lei Xie<sup>1</sup>, Mengxiao Bi<sup>2</sup></center>
    <center><a href="http://www.npu-aslp.org"><sup>1</sup>Audio, Speech and Language Processing Group (ASLP@NPU), School of Computer Science,</br>Northwestern Polytechnical University, Xi'an, China </a></center>
    <center><a href="https://fuxi.163.com/laboratory"><sup>2</sup>Fuxi AI Lab, NetEase Inc., Hangzhou, China </a></center>
    <center><a href="http://www.sribd.cn/en"><sup>3</sup>Shenzhen Research Institute of Big Data,</br>The Chinese University of Hong Kong, Shenzhen (CUHK-Shenzhen), China</a></center>

    <br><br>
    <h2 id="abstract">1. Abstract<a name="abstract"></a></h2>
    <p>Streaming voice conversion has become increasingly popular for its potential in real-time applications. The recently proposed DualVC 2 has achieved robust and high-quality streaming voice conversion with a latency of about 180ms. Nonetheless, the recognition-synthesis framework hinders end-to-end optimization, and the instability of automatic speech recognition (ASR) model with short chunks makes it challenging to further reduce latency. To address these issues, we propose an end-to-end model, DualVC 3. With speaker-independent semantic tokens to guide the training of the content encoder, the dependency on ASR is removed and the model can operate under extremely small chunks, with cascading errors eliminated. A language model is trained on the content encoder output to produce pseudo context by iteratively predicting future frames, providing more contextual information for the decoder to improve conversion quality. Experimental results demonstrate that DualVC 3 achieves comparable performance to DualVC 2 in subjective and objective metrics, with a latency of only 50 ms.
    </p>

    <table frame=void rules=none>
      <tr>
        <center><img src='raw/fig/overall.png'></center>
      </tr>
      <tr>
      </tr>
    </table>
    <br><br>

    <h2>2. Computational Metrics</h2>
    <ul>
      <table>
  <tbody>
    <tr>
      <td></td>
      <td>RTF</td>
      <td>Latency (ms)</td>
      <td>Params (M)</td>
    </tr>
    <tr>
      <td>Full mode</td>
      <td>0.797</td>
      <td>15.94+20+20=55.94</td>
      <td>22.7</td>
    </tr>
    <tr>
      <td>&emsp;AM (w/ 2 pseudo ctx)</td>
      <td>0.201</td>
      <td>4.02</td>
      <td>10.9</td>
    </tr>
    <tr>
      <td>&emsp;Vocoder (w/ 2 pseudo ctx)</td>
      <td>0.086</td>
      <td>1.72</td>
      <td>1.2</td>
    </tr>
    <tr>
      <td>&emsp;LM</td>
      <td>0.510</td>
      <td>10.20</td>
      <td>10.6</td>
    </tr>
    <tr>
      <td>Stand-alone mode</td>
      <td>0.181</td>
      <td>3.58+20+20=43.58</td>
      <td>12.1</td>
    </tr>
    <tr>
      <td>&emsp;AM (w/o pseudo ctx)</td>
      <td>0.134</td>
      <td>2.68</td>
      <td>10.9</td>
    </tr>
    <tr>
      <td>&emsp;Vocoder (w/o pseudo ctx)</td>
      <td>0.047</td>
      <td>0.90</td>
      <td>1.2</td>
    </tr>
  </tbody>
  <colgroup>
    <col>
    <col>
    <col>
    <col>
  </colgroup>
</table>
    </ul>

    <h2>3. Demo<a name="Comparison"></a></h2>
    <ul>
      <li>DualVC2: Streaming mode of DualVC 2 [1].</li>
      <li>VQMIVC: Non-streaming mode of VQMIVC [2].</li>
      <li>VQMIVC-streaming: Streaming mode of VQMIVC.</li>
      <li>DualVC3-full: Full mode of DualVC 3.</li>
      <li>DualVC3-standalone: Standalone mode of DualVC 3.</li>
    </ul>

    <table>
      <tbody id="tbody">
      </tbody>
    </table> 

    </br>
    <cite>[1] Z. Ning, Y. Jiang, P. Zhu, S. Wang, J. Yao, L. Xie, and M. Bi, “Dualvc 2: Dynamic masked convolution for unified streaming and non-streaming voice conversion,” in Proc. ICASSP. IEEE, 2024, pp. 1–5.</cite>
    </br>
    <cite>[2] D. Wang, L. Deng, Y. T. Yeung, X. Chen, X. Liu, and H. Meng, “VQMIVC: vector quantization and mutual information-based unsupervised speech representation disentanglement for one-shot voice conversion,” in Proc. INTERSPEECH. ISCA, 2021, pp. 1344–1348.</cite>
  </section>
</body>

</html>

<script type="" text/javascript>
  window.onload = function () {
    let scenes = ["Clean"]
    let speakers = ["female", "male"]
    let genders = ["female", "male"]
    let models = ["DualVC2", "VQMIVC", "VQMIVC-streaming", "DualVC3-full", "DualVC3-standalone"]
    let all_samples = [["SSB00430070.wav", "SSB02460443.wav", "SSB03230482.wav", "SSB03410353.wav", "SSB03750317.wav", "SSB04700097.wav", "SSB05900051.wav", "SSB07100255.wav", "SSB07100388.wav", "SSB08170086.wav",], 
                    ["SSB00430070.wav", "SSB02460443.wav", "SSB03230482.wav", "SSB03410353.wav", "SSB03750317.wav", "SSB04700097.wav", "SSB05900051.wav", "SSB07100255.wav", "SSB07100388.wav", "SSB08170086.wav",]]
    let sample_data = `
        <tr>
          <td style="text-align: center; width: 150px;" rowspan=2><strong>Target Speaker<strong></td>
          <td style="text-align: center; width: 150px;" rowspan=2><strong>Source speech<strong></td>
          <td style="text-align: center; width: 150px;" colspan=5><strong>Method<strong></td>
        </tr>
        <tr>
        `
    for (const id in models) {
      model = models[id]
      if (model == 'Baseline') {
        model = 'IBF-VC'
      }
      sample_data += '<td style="text-align: center; width: 150px;" rowspan=1><strong>' + model + '<strong></td>'
    }
    sample_data += "</tr>"
    console.log(sample_data)
    
    for (let x in scenes) {
      let scene = scenes[x]
      let scene_data = ""
      scene_data += '<tr>'
      //scene_data += '<td style="text-align: center; width: 150px;" rowspan=' + 8 + '><strong>' + scene + ' Source' + '<strong></td>'
      let samples = all_samples[x]
      console.log(scene, samples)
      for (let y in speakers) {
        speaker = speakers[y]
        gender = genders[y]
        scene_data += '<td style="text-align: center; width: 150px;" rowspan=10>' + gender + '<audio style="width: 150px;"controls="" src="raw/samples/speakers/' + speaker + '.wav"></td>'
        for (let z in samples) {
          if (z != 0) {
            scene_data += '<tr>'
          }
          let sample = samples[z]
          scene_data += '<td style="text-align: center"><audio style="width: 150px;" controls="" src="raw/samples/' + scene + '/source/' + sample + '"></audio></td>'
          for (let w in models) {
            let model = models[w]
            scene_data += '<td style="text-align: center"><audio style="width: 150px;" controls="" src="raw/samples/' + scene + '/' + model + '/' + speaker + '/' + sample + '"></audio></td>'
          }
          scene_data += '</tr>'
        }
      }
      sample_data += scene_data
    }
    document.getElementById('tbody').innerHTML = sample_data
  }
</script>
